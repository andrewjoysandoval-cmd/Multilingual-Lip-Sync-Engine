âœ¨ Multilingualâ€‘Lipâ€‘Syncâ€‘Engine (MLE)
Realâ€‘time, multilingual mouthâ€‘sync for humanoid robots.
Text â†’ TTS visemes â†’ smoothed servo motions â€” on an embedded SBC â€” with caching and fallbacks.

Built to slot into Realbotix Fâ€‘Series style heads. Works with Azure TTS viseme events; degrades gracefully when visemes arenâ€™t available.

ğŸ” Why this exists 
Realbotixâ€™s current public flow historically centered on English lipâ€‘sync; expanding to Spanish / Cantonese / Mandarin / French and more means the robot must map any languageâ€™s phonemes â†’ a fixed set of visemes, time them to the audio, and drive actuators smoothly in realâ€‘time. MLE does exactly that using Azure Neural TTS viseme events (and a forcedâ€‘alignment fallback when visemes arenâ€™t provided).

ğŸ§© What you get
Unified 22â€‘viseme schema (Microsoftâ€‘style) â†’ 10 normalized servo channels

Coarticulation blending (attack/decay crossâ€‘fades) for natural motion

Azure streaming support (viseme IDs + audio offsets in ticks)

File cache (audio + viseme timeline) to cut latency & cloud calls

Forcedâ€‘alignment fallback (simple, swappable for MFA or your aligner)

CLI + modular code (engine.py, tts.py, mapping.py, coarticulation.py, cache.py, fallback.py)

ğŸ—‚ï¸ Repo layout
bash
Copy
mle_demo/            # Minimal demo (fast evaluation)
mle_full_release/    # Production-ready engine (use this for integration)
âš™ï¸ Requirements
Python 3.10+

For Azure mode: an Azure Speech resource (Key + Region)

bash
Copy
python -m venv .venv && source .venv/bin/activate
pip install azure-cognitiveservices-speech pydub pandas
ğŸš€ Quick start
Demo (offline or Azure)
bash
Copy
# Offline (simulated visemes)
python mle_demo/mle_demo.py --text "Hola, Â¿cÃ³mo estÃ¡s?" --language es --output demo_es.csv

# Azure (live visemes)
python mle_demo/mle_demo.py \
  --text "Hello, how are you?" \
  --language en-US \
  --azure-key $AZURE_SPEECH_KEY \
  --azure-region $AZURE_SPEECH_REGION \
  --use-azure \
  --output demo_en.csv
Full Release (recommended)
bash
Copy
# Azure
python -m mle_full_release.main \
  --text "ä½ å¥½ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ " \
  --language zh-CN \
  --azure-key $AZURE_SPEECH_KEY \
  --azure-region $AZURE_SPEECH_REGION \
  --output out_zh.csv

# Offline / Simulated
python -m mle_full_release.main \
  --text "Hola, Â¿cÃ³mo estÃ¡s?" \
  --language es-ES \
  --output out_es.csv
Output format: CSV with time_ms, servo_0..servo_9.
Sample period defaults to 50â€¯ms (configurable).

ğŸ”§ Mapping to your head (Fâ€‘Series style)
Open mle_full_release/mapping.py.

Calibrate the 10 normalized channels [0.0..1.0] to your servo IDs & travel ranges.

Keep jaw safety limits conservative; increase gradually after visual checks.

Stream the CSV to your motion controller at the same sample period you used to generate it.

Tip: start with vowels (/A/ /E/ /I/ /O/ /U/) and bilabials (/P/ /B/ /M/). Tune lipâ€‘corner, pucker, jaw.

ğŸ§ª How coarticulation works 
MLE blends neighboring visemes with attack/decay windows so the mouth begins forming the next shape before the current one fully ends. This avoids â€œflappingâ€ and matches how human articulators overlap in time. Tweak the window (e.g., 50â€“120â€¯ms) in coarticulation.py for your servosâ€™ speed.

âš¡ Caching
Keyed by (text, language, voice).

Stores: audio.wav + visemes.json.

Turn it on with --cache-dir .mle_cache (CLI) or via LipSyncEngine(..., cache_dir=...).

ğŸ†˜ Fallback alignment
If your TTS doesnâ€™t emit viseme events, MLE can approximate timings over the audio duration. The included fallback is intentionally simple so you can swap in Montreal Forced Aligner (or any phoneme aligner) for production.

ğŸ–§ Realâ€‘time notes
Azure viseme events include ID + audio_offset (100â€‘ns ticks). We convert to ms and schedule servo targets accordingly.

Keep your audio output buffer predictable; introduce a small visual lead (e.g., ~40â€“60â€¯ms) if your audio path is slower than the motion loop.

ğŸ”’ Privacy & safety
Cache lives locally; clear it for sensitive phrases.

If you use face tracking / personalization elsewhere, ensure consent & retention policies suit your market.

ğŸ—ºï¸ Roadmap
Plugâ€‘in MFA for accurate phoneme timings (dropâ€‘in replacement for simplified fallback)

Export binary streaming format for controllers (Protobuf)

More viseme presets (12â€“14 channel rigs)

Sample ROS node & WebSocket streamer

Automated MOS and latency harness

ğŸ“ References 
Realbotix language expansion coverage (Business Wire)

Microsoft Learn â€” Visemes & viseme/phoneme groupings

Azure Speech SDK â€” Viseme events + audio_offset ticks

Coarticulation â€” overlapping gestures (intro)


ğŸ“„ License
MIT (or your preferred license). Add a LICENSE file.

ğŸ¤ Contributing
Issues and PRs welcome. Please include a short clip or CSV snippet with any bug report so we can reproduce timing problems.

ğŸ§° Maintainer shortcuts
Demo CSV quickâ€‘gen: python mle_demo/mle_demo.py â€¦

Full engine CSV: python -m mle_full_release.main â€¦

Clear cache: delete --cache-dir folder.

